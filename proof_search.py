import torch
import json
from transformers import T5Tokenizer, T5ForConditionalGeneration
from transformers import RobertaTokenizer, RobertaForSequenceClassification

path = "entailment_bank/data/public_dataset/entailment_trees_emnlp2021_data_v2/dataset/task_1/"

t5_tokenizer = T5Tokenizer.from_pretrained("t5-large")
roberta_tokenizer = RobertaTokenizer.from_pretrained("roberta-large")

device = 'cuda'

class Node:
    def __init__(self):
        self.in_bound_edges = []
        self.out_bound_edges = []
        self.type = None
        self.score = None


def proof_search(one_task, prover, verifier):
    greedy_proof = generate_greedy(one_task, prover)

def generate_greedy(one_task, prover, verifier):
    """
        return a proof for the task generated by the prover using a greedy way
    """
    import pdb
    pdb.set_trace()
    greedy_proof = []

    hypothesis_str = f"$hypothesis$ = {one_task['hypothesis']};"
    fact_str = f"$fact$ = {one_task['context']};"

    # initial partial proof is empty
    partial_proofs_str = "$partial_proof$ ="

    # generate one proof greadily
    input_str = f"{hypothesis_str} {fact_str} {partial_proofs_str}"

    input_ids = t5_tokenizer(input_str, return_tensors="pt").input_ids
    input_ids = input_ids.to(device)
    outputs = prover.generate(input_ids, output_scores=True, max_length=128)

    # parse the output
    greedy_proof.append(outputs)
    outputs = None



# graph is just represented by a collection of nodes







def eval_task(file_path, prover, verifier):
    with open(file_path, "r") as f:
        lines = f.readlines()
        for line in lines:
            one_task = json.loads(line)
            proof_search(one_task, prover, verifier)

if __name__ == "__main__":
    prover = T5ForConditionalGeneration.from_pretrained("./T5_larget_epoch_2/")
    verifier = None

    eval_task(path + "test.jsonl", prover, verifier)
